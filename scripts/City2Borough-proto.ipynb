{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "from pyspark.sql import Row, Column\n",
    "\n",
    "NEIGHBORHOODS_CSV_FPATH = 'data/wiki_Neighborhoods_in_New_York_City.csv'\n",
    "\n",
    "def import_neighborhoods(fname):\n",
    "    '''\n",
    "    import wikipedia neighborhoods file\n",
    "    returns: dictionary where key is borough\n",
    "        and value is list of neighborhoods\n",
    "    '''\n",
    "    with open(fname, 'r') as f:\n",
    "        raw = f.read()\n",
    "    \n",
    "    lines = raw.split(os.linesep)\n",
    "    \n",
    "    assert len(lines)==59, \"Neighborhoods file not proper length\"\n",
    "    \n",
    "    boroughs = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        fields = line.split(',')\n",
    "        borough = fields.pop(0)\n",
    "        if borough not in boroughs:\n",
    "            boroughs[borough]=[]\n",
    "        for f in fields:\n",
    "            if f:\n",
    "                neighborhood = str.upper(f.strip())\n",
    "                boroughs[borough].append(neighborhood)\n",
    "    boroughs_dict = {'QUEENS': set(boroughs['Queens']+['QUEENS']),\n",
    "                     'BROOKLYN': set(boroughs['Brooklyn']+['BROOKLYN']),\n",
    "                     'MANHATTAN': set(boroughs['Manhattan']+['MANHATTAN','NEW YORK']),\n",
    "                     'STATEN ISLAND': set(boroughs['Staten Island']+['STATEN ISLAND']),\n",
    "                     'BRONX':set(boroughs['Bronx']+['BRONX'])}\n",
    "    return boroughs_dict\n",
    "\n",
    "def _city2borough(borough, neighborhoods):\n",
    "    '''\n",
    "    If borough is in the borough_list, return it\n",
    "    Otherwise, apply the city-to-borough map\n",
    "    '''\n",
    "    orig_borough, city = borough_city_tuple\n",
    "    borough_list = ['QUEENS','BROOKLYN','BRONX','STATEN ISLAND','MANHATTAN']\n",
    "    try: #convert to upper if it's a string\n",
    "        if str.upper(orig_borough) in borough_list:\n",
    "            return (str.upper(orig_borough), city)\n",
    "    except: #if orig_borough not a string, ignore error\n",
    "        pass\n",
    "    else: #if \n",
    "        #Return first borough that comes up\n",
    "        for borough,hood_list in neighborhoods.items():\n",
    "            try:\n",
    "                if str.upper(city) in hood_list:\n",
    "                    return (borough, city)\n",
    "            except:\n",
    "                return (None, city)\n",
    "        return (None, city)\n",
    "\n",
    "neighborhoods = import_neighborhoods(NEIGHBORHOODS_CSV_FPATH)\n",
    "city2borough = partial(_city2borough, neighborhoods=neighborhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/311-all.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reduced_df = df.select(\n",
    "#     'Unique Key', \n",
    "#     'Created Date',\n",
    "#     'Complaint Type',\n",
    "#     'Incident Zip',\n",
    "#     'Incident Address',\n",
    "#     'City',\n",
    "#     'Borough',\n",
    "#     'Latitude',\n",
    "#     'Longitude'\n",
    "# )\n",
    "\n",
    "# reduced_df.write.csv(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_alt = as_rdd.map(lambda row: (row['Unique Key'], row['Borough'], row['City'])) \\\n",
    "               .map(city2borough) \\\n",
    "               .map(lambda pair: Row(Key=pair[0], Borough=pair[1], City=pair[2])) \\\n",
    "               .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn('Unique Key', df['Unique Key'].cast(IntegerType()))\n",
    "df_alt = df_alt.withColumn('Key', df_alt['Key'].cast(IntegerType()))\n",
    "\n",
    "df.createOrReplaceTempView('origin_df')\n",
    "df_alt.createOrReplaceTempView('new_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df = sqlContext.sql(\"\"\"\n",
    "        SELECT \n",
    "            origin_df.*, \n",
    "            new_df.Borough AS Borough_2 \n",
    "        FROM \n",
    "            origin_df \n",
    "            INNER JOIN new_df ON origin_df.`Unique Key` = new_df.`Key`\n",
    "    \"\"\")\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df = new_df.drop('Borough').withColumnRenamed('Borough_2', 'Borough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_borough(df):\n",
    "    \n",
    "    from pyspark.sql import IntegerType\n",
    "    \n",
    "    df_alt = (\n",
    "        df.rdd.map(lambda row: (row['Unique Key'], row['Borough'], row['City']))\n",
    "              .map(city2borough)\n",
    "              .map(lambda pair: Row(Key=pair[0], Borough=pair[1], City=pair[2]))\n",
    "              .toDF()\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn('Unique Key', df['Unique Key'].cast(IntegerType()))\n",
    "    df_alt = df_alt.withColumn('Key', df_alt['Key'].cast(IntegerType()))\n",
    "    \n",
    "    df.createOrReplaceTempView('origin_df')\n",
    "    df_alt.createOrReplaceTempView('new_df')\n",
    "    \n",
    "    new_df = sqlContext.sql(\"\"\"\n",
    "        SELECT \n",
    "            origin_df.*, \n",
    "            new_df.Borough AS Borough_2 \n",
    "        FROM \n",
    "            origin_df \n",
    "            INNER JOIN new_df ON origin_df.`Unique Key` = new_df.`Key`\n",
    "    \"\"\")\n",
    "    \n",
    "    new_df = new_df.drop('Borough').withColumnRenamed('Borough_2', 'Borough')\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df = clean_borough(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
